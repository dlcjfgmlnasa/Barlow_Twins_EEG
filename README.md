# Barlow_Twins_EEG
A Novel Negative-sample-free Contrastive Self-Supervised Learning for EEG-Based Motor Imagery Classification

>**In-Nea Wang, Cheol-Hui Lee, Hakseung Kim, and Dong-Joo Kim**

## Introduction ðŸ”¥
Motor imagery-based Brain-computer interfaces (MI-BCI) systems convert user intentions into computer commands, which are particularly useful in providing communication means and aiding rehabilitation for individuals with motor disabilities. Conventionally, MI classification research has focused on supervised learning to extract features from complex brain waves. However, the primary challenge of supervised learning approaches is in acquiring large volumes of reliably labeled high-quality data. Dependence on labeled data is restricted by specific experimental paradigms and protocols, making it challenging to ensure generalized high performance of the supervised learning-based models. To address these challenges, this study proposes a contrastive self-supervised learning (SSL) method for MI classification that does not require negative samples. Additionally, a backbone network called MultiResolutionCNN has been introduced, designed to consider various temporal and spatial scales of raw electroencephalogram (EEG) signals. Utilizing Barlow Twins loss-based contrastive SSL, features are effectively extracted from EEG signals without using negative samples and labels. MI classification was performed on two datasets not used in training: an in-domain dataset with two classes and an out-domain dataset with four classes, achieving accuracies of 81.56% and 42.36%, respectively. The results demonstrated superior performance compared to supervised learning methods. An ablation study compared MultiResolutionCNN with the baseline backbone networks and demonstrated excellent performance, especially when fine-tuning was applied. Additionally, the performance variations were observed with changes in hyperparameters. The maximum differences in accuracy for lambda, learning rate, and batch size were 1.11%, 3.58%, and 3.22%, respectively, indicating robust generalization of performance.

## Main Result ðŸ¥‡
#### Comparison with Other Supervised Learning Approaches
<table><thead>
  <tr>
    <th></th>
    <th colspan="3">OpenBMI</th>
    <th colspan="3">SingleArmMI</th>
  </tr></thead>
<tbody>
  <tr>
    <td></td>
    <td> <b> ACC </b> </td>
    <td> <b> MF1 </b> </td>
    <td> <b> Îº </b> </td>
    <td> <b> ACC </b> </td>
    <td> <b> MF1 </b> </td>
    <td> <b> Îº </b> </td>
  </tr>
  <tr>
    <td>FBCSP</td>
    <td>61.03Â±4.46</td>
    <td>60.5Â±14.55</td>
    <td>0.22Â±0.09</td>
    <td>24.86Â±1.76</td>
    <td>21.84Â±3.48</td>
    <td>-0.00Â±0.02</td>
  </tr>
  <tr>
    <td>ShallowConvNet</td>
    <td>78.95Â±5.19</td>
    <td>78.93Â±5.22</td>
    <td>0.58Â±0.10</td>
    <td>37.36Â±2.74</td>
    <td>36.23Â±3.22</td>
    <td>0.16Â±0.04</td>
  </tr>
  <tr>
    <td>DeepConvNet</td>
    <td>77.39Â±5.08</td>
    <td>77.37Â±5.08</td>
    <td>0.55Â±0.10</td>
    <td>36.39Â±1.64</td>
    <td>35.90Â±1.56</td>
    <td>0.15Â±0.02</td>
  </tr>
  <tr>
    <td>EEGNet</td>
    <td>79.45Â±4.68</td>
    <td>79.41Â±4.69</td>
    <td>0.59Â±0.09</td>
    <td>33.75Â±3.62</td>
    <td>33.76Â±3.67</td>
    <td>0.12Â±0.05</td>
  </tr>
  <tr>
    <td> <b> Proposed Model (linear-prob) </b> </td>
    <td>64.91Â±5.12</td>
    <td>64.82Â±5.18</td>
    <td>0.30Â±0.10</td>
    <td>34.58Â±4.53</td>
    <td>34.38Â±4.61</td>
    <td>0.13Â±0.06</td>
  </tr>
  <tr>
    <td> <b> Proposed Model (fine-tuning) </b> </td>
    <td><b>82.47Â±4.04</b></td>
    <td><b>82.45Â±4.05</b></td>
    <td><b>0.65Â±0.08</b></td>
    <td><b>43.19Â±3.92</b></td>
    <td><b>42.79Â±4.13</b></td>
    <td><b>0.24Â±0.05</b></td>
  </tr>
</tbody></table>

#### Semi-Supervised Learning
#### UMAP visualization & Confusion Matrix


## License and Citation ðŸ“°
The software is licensed under the Apache License 2.0. Please cite the following paper if you have used this code:

